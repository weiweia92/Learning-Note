{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is binary classification.\n",
    "\n",
    "## Logistic loss function\n",
    "\n",
    "$ L(\\hat{y}, y)=-(y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}) $  \n",
    "If y=1: $L(\\hat{y}, y)=-\\log \\hat{y}$ $\\longleftarrow$ want $\\log \\hat{y}$ large, want $\\hat{y}$ large  \n",
    "\n",
    "If y=0: $L(\\hat{y}, y)=-\\log (1-\\hat{y})$ $\\longleftarrow$ want $\\log 1-\\hat{y}$ large, want $\\hat{y}$ small   \n",
    "\n",
    "### Why cost function use cross entropy instead of MSE?\n",
    "Because MSE is not convex function,it has multiple local optimal solution,but cross entropy is convex function,it has only one optimal solution.\n",
    "\n",
    "### What is the difference between the cost function and the loss function for logistic regression?\n",
    "The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.  \n",
    "Want to find w,b that minimize cost function(J(w, b)).\n",
    "\n",
    "$ Cost function: J(W, b)=\\frac{1}{m}\\sum_i^m L(\\hat{y}^{(i)}, y^{(i)})=-\\frac{1}{m}\\sum_i^m [y^{(i)} \\log \\hat{y}^{(i)}+(1-y^{(i)}\\log(1-\\hat{y}^{(i)}))] $  \n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Repeat  \n",
    "$$\n",
    "\\begin{cases}\n",
    "W=W - \\alpha \\times \\frac{\\partial J(W,b)}{\\partial W} \\\\\\\\\n",
    "b=b - \\alpha \\times \\frac{\\partial J(W,b)}{\\partial b}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$\\alpha : learningrate$\n",
    "\n",
    "### Note!\n",
    "When writing code, we use dW instead of $\\frac{\\partial J(W,b)}{\\partial W}$, db instead of $\\frac{\\partial J(W,b)}{\\partial b}$  \n",
    "\n",
    "## Logistic regression derivatives(求导)\n",
    "\n",
    "### Backpropagation of one sample \n",
    "$$z=w_1x_1+w_2x_2+b\\rightarrow a=\\sigma{z} \\rightarrow L(a,y)$$  \n",
    "Because logistic regression is binary classification,so loss function is $$ L(a,y)=-(y\\log{a}+(1-y)\\log{(1-a)}) $$ \n",
    "`da` =$\\frac{L(a,y)}{da}$  \n",
    "\n",
    "`dz` =$\\frac{dL(a,y)}{dz}=\\frac{dL(a,y)}{da} \\frac{da}{dz}=(-\\frac{y}{a}+\\frac{1-y}{1-a}) a(1-a)=a-y$  \n",
    "\n",
    "`dw1` =$\\frac{\\partial L}{\\partial w_1}=\\frac{dL}{dz} \\frac{dz}{dw_1}=x_1 \\frac{dL(a,y)}{dz}$  \n",
    "\n",
    "`dw2` =$\\frac{\\partial L}{\\partial w_2}=\\frac{dL}{dz} \\frac{dz}{dw_2}=x_2 \\frac{dL(a,y)}{dz}$  \n",
    "\n",
    "`db` =$\\frac{\\partial L}{\\partial b}=\\frac{dL}{dz} \\frac{dz}{db}=1*\\frac{dL(a,y)}{dz}$  \n",
    "\n",
    "so repeat:  \n",
    "$w_1:=w_1-\\alpha dw_1$  \n",
    "$w_2:=w_2-\\alpha dw_2$  \n",
    "$b:=b-\\alpha db$\n",
    "\n",
    "### Backpropagation of m samples\n",
    "\n",
    "$J(W, b)=\\frac{1}{m}\\sum_i^m L(a^{(i)}, y^{(i)})$  \n",
    "$a^{(i)}=\\hat{y}^{(i)}=\\sigma{(z^{(i)})}=\\sigma(w^Tx^{(i)}+b)$,$x^{(i)}=\\left[\n",
    "\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$  \n",
    "$\\frac{J(w,b)}{w_1}=\\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_1}$, $code:dw_1^{(i)}= \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_1}$  \n",
    "\n",
    "$\\frac{J(w,b)}{w_2}=\\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_2}$, $code:dw_2^{(i)}= \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_2}$\n",
    "\n",
    "$\\frac{J(w,b)}{b}=\\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial b}$, $code:db^{(i)}= \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial b}$  \n",
    "\n",
    "Pseudo code  \n",
    "$J=0;dw_1=0;dw_2=0;db=0$  \n",
    "\n",
    "$for　i=1　to　m:$  \n",
    "　　$z^{(i)}=W^Tx^{(i)}+b$\n",
    "　　$a^{(i)}=\\sigma(z^{(i)})$  \n",
    "　　$J+=-(y^{(i)}\\log{a^{(i)}}+(1-y^{(i)})\\log{(1-a^{(i)})})$  \n",
    "　　$dz^{(i)}=a^{(i)}-y^{(i)}$  \n",
    "　　$dw_1+=x_1^{(i)}dz^{(i)}$  \n",
    "　　$dw_2+=x_2^{(i)}dz^{(i)}$  \n",
    "　　$db+=dz^{(i)}$ \n",
    "  \n",
    "$J/=m$\n",
    "\n",
    "$dw_1/=m$  \n",
    "\n",
    "$dw_2/=m$  \n",
    "\n",
    "$db/=m$  \n",
    "\n",
    "$dw_1=\\frac{\\partial J}{\\partial w_1}$    \n",
    "\n",
    "repeat:\n",
    "\n",
    "$w_1:=w_1-\\alpha dw_1$  \n",
    "\n",
    "$w_2:=w_2-\\alpha dw_2$  \n",
    "\n",
    "$b:=b-\\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
