{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is binary classification.\n",
    "\n",
    "## Logistic loss function\n",
    "\n",
    "$ L(\\hat{y}, y)=-(y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}) $  \n",
    "If y=1: $L(\\hat{y}, y)=-\\log \\hat{y}$ $\\longleftarrow$ want $\\log \\hat{y}$ large, want $\\hat{y}$ large  \n",
    "\n",
    "If y=0: $L(\\hat{y}, y)=-\\log (1-\\hat{y})$ $\\longleftarrow$ want $\\log 1-\\hat{y}$ large, want $\\hat{y}$ small   \n",
    "\n",
    "### How to get the logistic loss function?\n",
    "$If　y=1:　p(y|x)=\\hat{y}$  \n",
    "$If　y=0:　p(y|x)=1-\\hat{y}$\n",
    "$\\rightarrow p(y|x)=\\hat{y}^{y}(1-\\hat{y})^(1-y)$  \n",
    "\n",
    "Because the log function is a strictly monotonically increasing function, your maximizing $log p(y|x)$ should give you a similar result as optimizing $p(y|x)$.   \n",
    "\n",
    "$$\\log{p(y|x)}=\\log \\hat{y}^{y} (1-\\hat{y})^{(1-y)}=y \\log(\\hat{y})+(1-y)\\log(1-\\hat{y})$$  \n",
    "\n",
    "And so this is actually negative of the loss function that we had to find previously. And there's a negative sign there because usually if you're training a learning algorithm, you want to make probabilities large whereas in logistic regression we're expressing this. We want to minimize the loss function.   \n",
    "\n",
    "**Cost on m examples**  \n",
    "$$\\log p(labels　in　training set)=\\log \\quad \\prod_{i=1}^m p(y^{(i)}|x^{(i)}) \\quad $$\n",
    "$$\\log p(labels　in　training set)=\\sum_i^m \\log(p(y^{(i)}|x^{i}))＝-\\sum_i^{m}L(\\hat{y}^{(i)},y^{(i)})=-m \\times Cost　function $$  \n",
    "\n",
    "We're really carrying out maximum likelihood estimation(最大似然估计) with the logistic regression model. Under the assumption that our training examples were IID, or identically independently distributed. \n",
    "### Why cost function use cross entropy instead of MSE?\n",
    "Because MSE is not convex function,it has multiple local optimal solution,but cross entropy is convex function,it has only one optimal solution.\n",
    "\n",
    "### What is the difference between the cost function and the loss function for logistic regression?\n",
    "The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.  \n",
    "Want to find w,b that minimize cost function(J(w, b)).\n",
    "\n",
    "$ Cost function: J(W, b)=\\frac{1}{m}\\sum_i^m L(\\hat{y}^{(i)}, y^{(i)})=-\\frac{1}{m}\\sum_i^m [y^{(i)} \\log \\hat{y}^{(i)}+(1-y^{(i)}\\log(1-\\hat{y}^{(i)}))] $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Repeat  \n",
    "$$\n",
    "\\begin{cases}\n",
    "W=W - \\alpha \\times \\frac{\\partial J(W,b)}{\\partial W} \\\\\\\\\n",
    "b=b - \\alpha \\times \\frac{\\partial J(W,b)}{\\partial b}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$\\alpha : learningrate$\n",
    "\n",
    "### Note!\n",
    "When writing code, we use dW instead of $\\frac{\\partial J(W,b)}{\\partial W}$, db instead of $\\frac{\\partial J(W,b)}{\\partial b}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression derivatives(求导)\n",
    "\n",
    "### Backpropagation of one sample \n",
    "$$z=w_1x_1+w_2x_2+b\\rightarrow a=\\sigma{z} \\rightarrow L(a,y)$$  \n",
    "Because logistic regression is binary classification,so loss function is $$ L(a,y)=-(y\\log{a}+(1-y)\\log{(1-a)}) $$ \n",
    "`da` =$\\frac{L(a,y)}{da}$  \n",
    "\n",
    "`dz` =$\\frac{dL(a,y)}{dz}=\\frac{dL(a,y)}{da} \\frac{da}{dz}=(-\\frac{y}{a}+\\frac{1-y}{1-a}) a(1-a)=a-y$  \n",
    "\n",
    "`dw1` =$\\frac{\\partial L}{\\partial w_1}=\\frac{dL}{dz} \\frac{dz}{dw_1}=x_1 \\frac{dL(a,y)}{dz}$  \n",
    "\n",
    "`dw2` =$\\frac{\\partial L}{\\partial w_2}=\\frac{dL}{dz} \\frac{dz}{dw_2}=x_2 \\frac{dL(a,y)}{dz}$  \n",
    "\n",
    "`db` =$\\frac{\\partial L}{\\partial b}=\\frac{dL}{dz} \\frac{dz}{db}=1*\\frac{dL(a,y)}{dz}$  \n",
    "\n",
    "so repeat:  \n",
    "$w_1:=w_1-\\alpha dw_1$  \n",
    "$w_2:=w_2-\\alpha dw_2$  \n",
    "$b:=b-\\alpha db$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation of m samples\n",
    "\n",
    "$J(W, b)=\\frac{1}{m}\\sum_i^m L(a^{(i)}, y^{(i)})$  \n",
    "$a^{(i)}=\\hat{y}^{(i)}=\\sigma{(z^{(i)})}=\\sigma(w^Tx^{(i)}+b)$,$x^{(i)}=\\left[\n",
    "\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_{n_x} \\\\\n",
    "\\end{matrix}\n",
    "\\right],a^{(i)}=\\left[\\begin{matrix}\n",
    "a_1^{(i)} \\\\\n",
    "a_2^{(i)} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n_x}^{(i)} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$  \n",
    "$\\frac{J(w,b)}{w_1}=\\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_1}$, $code:dw_1^{(i)}= \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_1}$  \n",
    "\n",
    "$\\frac{J(w,b)}{w_2}=\\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_2}$, $code:dw_2^{(i)}= \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial w_2}$\n",
    "\n",
    "$\\frac{J(w,b)}{b}=\\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial b}$, $code:db^{(i)}= \\frac{\\partial L(a^{(i)},y^{(i)})}{\\partial b}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code  \n",
    "$J=0;dw_1=0;dw_2=0;db=0$  \n",
    "\n",
    "$for　i=1　to　m:$  \n",
    "　　$z^{(i)}=W^Tx^{(i)}+b$\n",
    "　　$a^{(i)}=\\sigma(z^{(i)})$  \n",
    "　　$J+=-(y^{(i)}\\log{a^{(i)}}+(1-y^{(i)})\\log{(1-a^{(i)})})$  \n",
    "　　$dz^{(i)}=a^{(i)}-y^{(i)}$  \n",
    "　　$dw_1+=x_1^{(i)}dz^{(i)}$  \n",
    "　　$dw_2+=x_2^{(i)}dz^{(i)}$  \n",
    "　　$db+=dz^{(i)}$ \n",
    "  \n",
    "$J/=m$\n",
    "\n",
    "$dw_1/=m$  \n",
    "\n",
    "$dw_2/=m$  \n",
    "\n",
    "$db/=m$  \n",
    "\n",
    "$dw_1=\\frac{\\partial J}{\\partial w_1}$    \n",
    "\n",
    "repeat:\n",
    "\n",
    "$w_1:=w_1-\\alpha dw_1$  \n",
    "\n",
    "$w_2:=w_2-\\alpha dw_2$  \n",
    "\n",
    "$b:=b-\\alpha db$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can change it above to vectorization.**  \n",
    "\n",
    "Pseudo code  \n",
    "$J=0;dw=np.zeros((n_x,1));db=0$  \n",
    "\n",
    "$for　i=1　to　m:$  \n",
    "　　$z^{(i)}=W^Tx^{(i)}+b$\n",
    "　　$a^{(i)}=\\sigma(z^{(i)})$  \n",
    "　　$J+=-(y^{(i)}\\log{a^{(i)}}+(1-y^{(i)})\\log{(1-a^{(i)})})$  \n",
    "　　$dz^{(i)}=a^{(i)}-y^{(i)}$  \n",
    "　　$dw+=x^{(i)}dz^{(i)}$  \n",
    "　　$db+=dz^{(i)}$ \n",
    "  \n",
    "$J/=m$\n",
    "\n",
    "$dw/=m$\n",
    "\n",
    "$db/=m$  \n",
    "\n",
    "$dw_1=\\frac{\\partial J}{\\partial w_1}$    \n",
    "\n",
    "repeat:\n",
    "\n",
    "$w:=w-\\alpha dw$    \n",
    "\n",
    "$b:=b-\\alpha db$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3, 4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[3 4]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([1, 2])\n",
    "print(a1)\n",
    "b1 = np.array([3, 4])\n",
    "print(b1)\n",
    "\n",
    "c1 = np.dot(a1,b1)\n",
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40737197, 0.98558692, 0.18633978, ..., 0.38695059, 0.07289723,\n",
       "       0.3516798 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1000000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250092.56167622298\n",
      "Vectorized version:0.5388259887695312ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c =  np.dot(a, b)\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print('Vectorized version:' + str(1000*(toc-tic))+'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250092.56167622848\n",
      "For loop:445.69945335388184ms\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print('For loop:'+str(1000*(toc-tic))+'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU and CPU have parallelization instructions(并行化指令). They're sometimes called **SIMD** instructions. This stands for a single instruction multiple data. But what this basically means is that, if you use built-in functions such as this np.function or other functions that don't require you explicitly implementing a for loop. It enables Phyton to take much better advantage of parallelism to do your computations much faster. And this is true both computations on CPUs and computations on GPUs. It's just that GPUs are remarkably good at these SIMD calculations but CPU is actually also not too bad at that. Maybe just not as good as GPUs. You're seeing how vectorization can significantly speed up your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Logistic Regression \n",
    "\n",
    "It trains without for loop!  \n",
    "\n",
    "$\n",
    "X = \\left[\n",
    "\\begin{matrix}\n",
    "　\\vdots　　\\vdots　 \\cdots　　\\vdots      \\\\\n",
    "　　x^{(1)}　x^{(2)}\\cdots　　x^{(m)}\\\\\n",
    "　\\vdots　　\\vdots 　\\cdots　　\\vdots \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$　　$X \\in (n_x, m)$   \n",
    "$Z=\\left[\n",
    "\\begin{matrix}\n",
    "z^{(1)}　z^{(2)}　\\cdots　z^{(m)} \\\\\n",
    "\\end{matrix}\n",
    "\\right]$ $=W^TX+ $ $\\left[\n",
    "\\begin{matrix}\n",
    "b　b　\\cdots b 　　\\\\\n",
    "\\end{matrix}\n",
    "\\right]$ $=\\left[\n",
    "\\begin{matrix}\n",
    "W^TX^{(1)}+b　W^TX^{(2)}+b　\\cdots　W^TX^{(m)}+b 　　\\\\\n",
    "\\end{matrix}\n",
    "\\right] $  \n",
    "$A=\\left[\n",
    "\\begin{matrix}\n",
    "a^{(1)}　a^{(2)}　\\cdots a^{(m)} 　　\\\\\n",
    "\\end{matrix}\n",
    "\\right]=\\sigma(Z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#code`  \n",
    "`Z = np.dot(w.T,X)+b`  \n",
    "\n",
    "上面提到:  \n",
    "$dz^{(i)}=a^{(i)}-y^{(i)}$  \n",
    "$dZ=\\left[\n",
    "\\begin{matrix}\n",
    "dz^{(1)}　dz^{(2)}　\\cdots dz^{(m)} 　　\\\\\n",
    "\\end{matrix}\n",
    "\\right]$\n",
    "$A=\\left[\n",
    "\\begin{matrix}\n",
    "a^{(1)}　a^{(2)}　\\cdots a^{(m)} 　　\\\\\n",
    "\\end{matrix}\n",
    "\\right]$  \n",
    "$Y=\\left[\n",
    "\\begin{matrix}\n",
    "y^{(1)}　y^{(2)}　\\cdots y^{(m)} 　　\\\\\n",
    "\\end{matrix}\n",
    "\\right]$\n",
    "$\\rightarrow dZ=A-Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面提到:  \n",
    "$dw+=x^{(i)}dz^{(i)}$  \n",
    "\n",
    "$db+=dz^{(i)}$\n",
    "\n",
    "$dw/=m$\n",
    "\n",
    "$db/=m$  \n",
    "\n",
    "so $\\rightarrow$  \n",
    "$dW = \\frac{1}{m}XdZ^T=\\frac{1}{m}\\left[\n",
    "\\begin{matrix}\n",
    "　\\vdots　　\\vdots　 \\cdots　　\\vdots      \\\\\n",
    "　　x^{(1)}　x^{(2)}\\cdots　　x^{(m)}\\\\\n",
    "　\\vdots　　\\vdots 　\\cdots　　\\vdots \\\\\n",
    "\\end{matrix}\n",
    "\\right]\\left[\n",
    "\\begin{matrix}\n",
    "dz^{(1)}　dz^{(2)}　\\cdots dz^{(m)} 　　\\\\\n",
    "\\end{matrix}\n",
    "\\right]=\\frac{1}{m}\\left[\n",
    "\\begin{matrix}\n",
    "x^{(1)}dz^{(1)}+x^{(2)}dz^{(2)}+\\cdots x^{(m)}dz^{(m)} 　　\\\\\n",
    "\\end{matrix}\n",
    "\\right],dW \\in(n \\times 1)$  \n",
    "\n",
    "$db=\\frac{1}{m}\\sum_{i=1}^{m}dz^{(i)}$\n",
    "`#code:`  \n",
    "`1/m np.sum(dZ)`  \n",
    "\n",
    "#### Summary \n",
    "`for iter in range(1000):`  \n",
    "\n",
    "　　　`Z = np.dot(W.T,X)+b`  \n",
    "　　　`A = np.exp(Z)`  \n",
    "　　　`dZ = A-Y`  \n",
    "　　　`dW = 1/m*X*dZ.T`  \n",
    "　　　`db = 1/m*np.sum(dZ)`  \n",
    "　　　`W = W - lr*dW`  \n",
    "　　　`b = b - lr*db`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-Numpy vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8669049   0.24053453  0.51378543 -1.61745633  0.62625261]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape \n",
    "# this is called a rank 1 array in Python and it's neither a row vector nor a column vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8669049   0.24053453  0.51378543 -1.61745633  0.62625261]\n"
     ]
    }
   ],
   "source": [
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.81552354351696\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a is same with a.T so what I would recommend is that when you're coding new networks, that you just not use data structures where the shape is (5,), or (n,), rank 1 array. Instead, if you set a to be this, (5,1), then this commits a to be (5,1) column vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19903633]\n",
      " [ 0.08218999]\n",
      " [ 0.13970819]\n",
      " [ 0.77363965]\n",
      " [-1.00748745]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5,1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19903633  0.08218999  0.13970819  0.77363965 -1.00748745]]\n"
     ]
    }
   ],
   "source": [
    "print(a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data structure above, `np.random.randn(5,1).T`there are two square brackets when we print a transpose. Whereas previously, `np.random.randn(5).T`there was one square bracket. So that's the difference between this is really a 1 by 5 matrix versus one of these rank 1 arrays.  \n",
    "\n",
    "Use `assert(a.shape == (5, 1))` to make sure this is a vector which you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03961546  0.01635879  0.027807    0.15398239 -0.2005266 ]\n",
      " [ 0.01635879  0.0067552   0.01148262  0.06358544 -0.08280539]\n",
      " [ 0.027807    0.01148262  0.01951838  0.10808379 -0.14075425]\n",
      " [ 0.15398239  0.06358544  0.10808379  0.5985183  -0.77943223]\n",
      " [-0.2005266  -0.08280539 -0.14075425 -0.77943223  1.01503096]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(a.shape == (5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(4, 3)\n",
    "b = np.random.randn(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,3) (3,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5d6d9d785276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,3) (3,2) "
     ]
    }
   ],
   "source": [
    "c = a*b #Note! n numpy the \"*\" operator indicates element-wise multiplication. It is different from \"np.dot()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3, 3)\n",
    "b = np.random.randn(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06921615, -0.13919856,  0.04555526],\n",
       "       [-0.61205946, -0.97378702,  0.67751549],\n",
       "       [-0.01218801, -0.05809639,  0.09167936]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
