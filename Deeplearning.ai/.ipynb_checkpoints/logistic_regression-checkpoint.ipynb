{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic loss function\n",
    "\n",
    "$ L(\\hat{y}, y)=-(y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})}) $  \n",
    "If y=1: $L(\\hat{y}, y)=-\\log \\hat{y}$ $\\longleftarrow$ want $\\log \\hat{y}$ large, want $\\hat{y}$ large  \n",
    "\n",
    "If y=0: $L(\\hat{y}, y)=-\\log (1-\\hat{y})$ $\\longleftarrow$ want $\\log 1-\\hat{y}$ large, want $\\hat{y}$ small   \n",
    "\n",
    "### What is the difference between the cost function and the loss function for logistic regression?\n",
    "The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.  \n",
    "Want to find w,b that minimize cost function(J(w, b)).\n",
    "\n",
    "$ Cost function: J(W, b)=\\frac{1}{m}\\sum_i^m L(\\hat{y}^{(i)}, y^{(i)})=-\\frac{1}{m}\\sum_i^m [y^{(i)} \\log \\hat{y}^{(i)}+(1-y^{(i)}\\log(1-\\hat{y}^{(i)}))] $  \n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Repeat  \n",
    "$$\n",
    "\\begin{cases}\n",
    "W=W - \\alpha \\times \\frac{\\partial J(W,b)}{\\partial W} \\\\\\\\\n",
    "b=b - \\alpha \\times \\frac{\\partial J(W,b)}{\\partial b}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$\\alpha : learningrate$\n",
    "\n",
    "### Note!\n",
    "When writing code, we use dW instead of $\\frac{\\partial J(W,b)}{\\partial W}$, db instead of $\\frac{\\partial J(W,b)}{\\partial b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
