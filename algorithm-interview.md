## 算法工程师面试

### 深度学习 

#### 模型评估方法

**Accuracy作为指标有哪些局限性？**

当正负样本极度不均衡时存在问题！比如，正样本有99%时，分类器只要将所有样本划分为正样本就可以达到99%的准确率。但显然这个分类器是存在问题的。当正负样本不均衡时，常用的评价指标为ROC曲线和PR曲线。

**ROC曲线和PR曲线各是什么？**

ROC:由FPR(False positive rate)和TPR(True positive rate)为横纵坐标的曲线  

FPR:FP/(FP+TN)误分为正样本的负样本占所有实际负样本的比例  
TPR:TP/(TP+FN)正确分类的正样本占所有实际正样本的比例  

True Positive, TP: 正确的正样本(预测为正样本，实际也为正样本)  
False Positive, FP:错误的正样本(预测为正样本，实际为负样本)  
True Negative, TN: 正确的负样本(预测为负样本，实际也为负样本)  
False Negative, FN: 错误的负样本(预测为负样本，实际为正样本)  

PR:Precision-Recall曲线，x:Recall，y:Precision  

Precision=TP/(TP+FP), 正确的正样本占所有预测正样本的比例  
Recall=TP/(TP+FN)， 正确的正样本占所有实际正样本的比例  

ROC越靠左上角效果越好，AUC为横纵坐标围成的面积，越大越好.  

**F1 Score**

F1=2\*precision\*recall/(precision+recall)  

**编程实现AUC的计算，并指出复杂度？**

**AUC指标有什么特点？放缩结果对AUC是否有影响？**

**余弦距离与欧式距离有什么特点？**

### 基本方法

- 什么是偏差和方差？

**什么是过拟合？深度学习解决过拟合的方法有哪**

过拟合表现在训练好的模型在训练集上效果很好，但是在测试集上效果差。也就是说模型的泛化能力弱.  
过拟合主要两个原因造成，数据太少和模型太复杂  
解决过拟合方法:  
early stopping  
l1 regularization:
l2 regularization:
dropout:通过修改隐藏层神经元的个数来防止网络的过拟合，也就是通过修改深度网络本身。在每一批次数据被训练时，Dropout按照给定的概率P随机剔除一些神经元，只有没有被剔除神经元的参数被更新。**每一批次数据，由于随机性剔除神经元，使得网络具有一定的稀疏性，从而能减轻了不同特征之间的协同效应。而且由于每次被剔除的神经元不同，所以整个网络神经元的参数也只是部分被更新，消除减弱了神经元间的联合适应性，增强了神经网络的泛化能力和鲁棒性。Dropout只在训练时使用，作为一个超参数，然而在测试集时，并不能使用**


**解决欠拟合的方法有哪些？**  

通过增加网络复杂度或者在模型中增加特征

- 深度模型参数调整的一般方法论？

### 优化方法

**简述了解的优化器，发展综述？**

BGD,SGD,动量优化法:Monentum,NAG(Nesterov accelerated gradient),自适应学习率优化法:Adagrad,RMSprop,AdaDelta,Adam


**常用的损失函数有哪些？分别适用于什么场景？**

$Focal loss=-\alpha_t(1-p_t)^\gamma \log(p_t)$  
交叉熵损失函数:$-\sum_{i}^{n} {y^i}\log(P_i)$
smoothl1  
MSE  

**梯度下降与拟牛顿法的异同？**

梯度下降用于数据较大情况，缺点:灭一步可能不是总向着最优解的方向  
拟牛顿:收敛速度更快，缺点:迭代时间长，需要计算一阶二阶导数  

### 深度学习基础

**以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播**

**NN的权重参数能否初始化为0？**

神经网络的权重w 的不同代表输入的向量有不同的特征，即权重越大的特征越重要，比如在人脸识别中，人脸的属性有眼睛，鼻子，嘴巴，眉毛，其中眼睛更能够影响人脸的识别，所以我们给与眼睛更大的权重。如果将权重初始化全为0，那么隐藏层的各个神经元的结果都是一样的，从而正向传播的结果是一样的，反向传播求得的梯度也是一样的，也就是说不管经过多少次迭代，更新的w(i)是相同的，这样就判断不了哪个特征比较重要了。因此，初始w不同，可以学到不同的特征，如果都是0或某个值，由于计算方式相同，可能达不到学习不同特征的目的

**常用的激活函数，导数？**

sigmoid,tanh,relu,leaky relu,swish,mish
$sigmoid=\frac{1}{1+e^{-x}}$  
$tanh=\frac{e^x-e^{-x}}{e^x+e^{-x}}$  
$swish=$
sigmoid,tanh为什么会导致梯度消失? 略  

- relu的有优点？又有什么局限性？他们的系列改进方法是啥？
- dropout和BN 在前向传播和方向传播阶段的区别？

### CNN

**给定卷积核的尺寸，特征图大小计算方法？**

**卷积层参数量，全连接层参数量，卷积层计算量，全连接层计算量**

**1\*1卷积层作用**

1.降维(dimension reductionality):eg,一张500\*500且厚度depth为100 的图片在20个filter上做1\*1的卷积，那么结果的大小为500\*500\*20,减少计算量,当然也可以升维，1x1卷积的作用是为了让网络根据需要能够更灵活的控制数据的depth的。  
2.加入非线性。卷积层之后经过激励层，1\*1的卷积在前一层的学习表示上添加了non-linear activation,提升网络的表达能力；

**增大感受野的方法**:较大的卷积核尺寸的卷积操作，空洞卷积，池化操作  

**常用的池化操作有哪些?有什么特点?**


- 网络容量计算方法

- 共享参数有什么优点

- CNN如何用于文本分类？

- resnet提出的背景和核心理论是？

- 空洞卷积是什么？有什么应用场景？

### RNN

- 简述RNN，LSTM，GRU的区别和联系

- 画出lstm的结构图，写出公式

- RNN的梯度消失问题？如何解决？

- lstm中是否可以用relu作为激活函数？

- lstm各个门分别使用什么激活函数？

- 简述seq2seq模型？

- seq2seq在解码时候有哪些方法？

- Attention机制是什么？



## 机器学习

### 基础

- 样本不均衡如何处理？
- 什么是生成模型什么是判别模型？

### 集成学习

- 集成学习的分类？有什么代表性的模型和方法？
- 如何从偏差和方差的角度解释bagging和boosting的原理？
- GBDT的原理？和Xgboost的区别联系？
- adaboost和gbdt的区别联系？

### 模型 

- 手推LR、Kmeans、SVM
- 简述ridge和lasson的区别和联系
- 树模型如何调参
- 树模型如何剪枝？
- 是否存一定存在参数，使得SVM的训练误差能到0
- 逻辑回归如何处理多分类？
- 决策树有哪些划分指标？区别与联系？
- 简述SVD和PCA的区别和联系？
- 如何使用梯度下降方法进行矩阵分解？
- LDA与PCA的区别与联系？

### 特征工程 

- 常用的特征筛选方法有哪些？
- 文本如何构造特征？
- 类别变量如何构造特征？
- 连续值变量如何构造特征？
- 哪些模型需要对特征进行归一化？
- 什么是组合特征？如何处理高维组合特征？
